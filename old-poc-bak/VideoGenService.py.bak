import json
import os
import time
import subprocess
import logging
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
from urllib import request as url_request
import glob
import random
from dotenv import load_dotenv
import pyrebase
import firebase_admin
from firebase_admin import credentials, firestore

from services.narration_service import generate_narration, estimate_text_duration, adjust_text_for_duration
from services.music_service import generate_music_score, add_background_music
from services.firebase_service import validate_firebase_connections, upload_video_to_firebase, update_firestore_with_video_url
from services.media_service import merge_video_audio, concatenate_videos

# Load environment variables
load_dotenv()

# Firebase Configuration for Storage
config = {
    "apiKey": os.getenv("FIREBASE_API_KEY"),
    "authDomain": os.getenv("FIREBASE_AUTH_DOMAIN"),
    "projectId": os.getenv("FIREBASE_PROJECT_ID"),
    "storageBucket": os.getenv("FIREBASE_STORAGE_BUCKET"),
    "messagingSenderId": os.getenv("FIREBASE_MESSAGING_SENDER_ID"),
    "appId": os.getenv("FIREBASE_APP_ID"),
    "measurementId": os.getenv("FIREBASE_MEASUREMENT_ID"),
    "databaseURL": f"https://{os.getenv('FIREBASE_PROJECT_ID')}.firebaseio.com"  # Required by pyrebase for storage
}

# Initialize Firebase Storage
firebase = pyrebase.initialize_app(config)
storage = firebase.storage()

# Initialize Firebase Admin for Firestore
cred = credentials.Certificate("deepflix-cc642-firebase-adminsdk-fbsvc-140547cc0d.json")
firebase_admin.initialize_app(cred)
db = firestore.client()

# Validate Firebase connectivity
print("\nüîç Validating Firebase connections...")

# Test storage connection
try:
    storage.child('test').get_url(None)
    print("‚úÖ Firebase Storage connection successful")
except Exception as e:
    print(f"‚ùå Firebase Storage connection failed: {str(e)}")
    raise Exception("Failed to connect to Firebase Storage")

# Test Firestore connection
try:
    db.collection('movies').limit(1).get()
    print("‚úÖ Firestore connection successful")
except Exception as e:
    print(f"‚ùå Firestore connection failed: {str(e)}")
    raise Exception("Failed to connect to Firestore")

print("‚úÖ Firebase initialization complete\n")

# API Config
COMFYUI_API_URL = "http://127.0.0.1:8188/prompt"
COMFYUI_BASE_DIR = os.path.expanduser("~/Desktop/ComfyUI")
COMFYUI_OUTPUT_DIR = os.path.join(COMFYUI_BASE_DIR, "output", "output")
TTS_API_URL = "http://localhost:5010/generate-voice"
MUSIC_GEN_API_URL = "http://localhost:5009/generate"
VIDEO_GENERATION_TIMEOUT = 1800  # 30 minutes timeout

app = Flask(__name__)
CORS(app)

# Validate Firebase connections on startup
validate_firebase_connections()

def format_sequence_number(num):
    """Formats a number into a 4-digit string (e.g., 1 -> '0001')."""
    return f"{num:04d}"

def check_video(output_folder, expected_video):
    """Check if a video exists and is valid."""
    # Check for all possible video patterns we've seen
    video_patterns = [
        os.path.join(output_folder, f"{expected_video}.mp4"),
        os.path.join(output_folder, f"{expected_video}_00001.mp4"),
        os.path.join(output_folder, f"{expected_video}__00001.mp4")
    ]
        
    for video_path in video_patterns:
        if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
            print(f"‚úÖ Found valid video: {os.path.basename(video_path)}")
            return True
                
    return False

def build_video_workflow(image_path, clip_action, output_folder, clip_duration=3.0625, transition_type="none"):
    """Constructs the ComfyUI workflow for video generation."""
    # Cap clip_duration at 6 seconds to prevent OOM
    clip_duration = min(float(clip_duration), 6.0)
    
    # Calculate frames based on duration + 1 second buffer
    FPS = 24
    buffered_duration = clip_duration + 0.5  # Add 0.5 second buffer
    num_frames = int(buffered_duration * FPS)
    
    # Get the base filename without extension
    base_filename = os.path.splitext(os.path.basename(image_path))[0]
    
    addFix = "consistent lighting throughout, well-lit scene, maintained brightness, "
    # Define prompts
    positive_prompt = addFix  + clip_action 
    
    # Optimized negative prompts with higher weights for critical issues
    negative_prompt = (
        "darkening, underexposed, fading to black, vignetting"
        # Critical face/hand issues (highest priority)
        "(distorted face:1.8), (melting face:1.8), (morphed face:1.8), "
        "(distorted hands:1.8), (extra fingers:1.8), (missing fingers:1.8), "
        
        # Movement and temporal issues
        "(unnatural movement:1.6), (jittering:1.6), (warping:1.6), "
        "(temporal inconsistency:1.6), (frame artifacts:1.6), "
        
        # B-roll specific issues
        "(blurry background:1.7), (wavy background:1.7), "
        "(color bleeding:1.6), (lighting inconsistency:1.6), "
        
        # General quality issues
        "(worst quality:1.4), (low quality:1.4), (blurry:1.4), "
        "(artifacts:1.4), (pixelated:1.4), "
        
        # Style and composition
        "(bad composition:1.3), (poor framing:1.3), "
        "(unnatural colors:1.3), (oversaturated:1.3), "
        
        # Safety
        "(text:1.2), (watermark:1.2), (logo:1.2)"
    )

    # Log the prompts and timing
    print("\nüé¨ Video Generation Prompts:")
    print(f"Positive Prompt: {positive_prompt}")
    print(f"Negative Prompt: {negative_prompt}")
    print(f"Original Duration: {clip_duration}s")
    print(f"Buffered Duration: {buffered_duration}s")
    print(f"Total Frames: {num_frames}")
    
    workflow = {
        "20": {
            "inputs": {
                "clip_name": "t5/google_t5-v1_1-xxl_encoderonly-fp8_e4m3fn.safetensors",
                "type": "sd3",
                "device": "default"
            },
            "class_type": "CLIPLoader"
        },
        "30": {
            "inputs": {
                "prompt": positive_prompt,
                "strength": 1,
                "force_offload": False,
                "clip": ["20", 0]
            },
            "class_type": "CogVideoTextEncode"
        },
        "31": {
            "inputs": {
                "prompt": negative_prompt,
                "strength": 1,
                "force_offload": True,
                "clip": ["30", 1]
            },
            "class_type": "CogVideoTextEncode"
        },
        "36": {
            "inputs": {
                "image": image_path
            },
            "class_type": "LoadImage"
        },
        "37": {
            "inputs": {
                "width": 1024,
                "height": 576,
                "upscale_method": "lanczos",
                "keep_proportion": True,
                "divisible_by": 16,
                "crop": "disabled",
                "image": ["36", 0]
            },
            "class_type": "ImageResizeKJ"
        },
        "44": {
            "inputs": {
                "frame_rate": FPS,
                "loop_count": 0,
                "filename_prefix": os.path.join(output_folder, base_filename),
                "format": "video/h264-mp4",
                "pix_fmt": "yuv420p",
                "crf": 5,              # Changed from 19 to 5 for better quality
                "save_metadata": True,
                "trim_to_audio": False,
                "pingpong": False,
                "save_output": True,
                "save_frames": False,
                "images": ["65", 0]    # Changed from 60 to 65 to use RIFE output
            },
            "class_type": "VHS_VideoCombine"
        },
        "59": {
            "inputs": {
                "model": "kijai/CogVideoX-5b-1.5-I2V",
                "precision": "bf16",
                "quantization": "disabled",
                "enable_sequential_cpu_offload": False,
                "attention_mode": "sdpa",
                "load_device": "main_device"
            },
            "class_type": "DownloadAndLoadCogVideoModel"
        },
        "60": {
            "inputs": {
                "enable_vae_tiling": True,
                "tile_sample_min_height": 240,
                "tile_sample_min_width": 360,
                "tile_overlap_factor_height": 0.2,
                "tile_overlap_factor_width": 0.2,
                "auto_tile_size": True,
                "vae": ["59", 1],
                "samples": ["63", 0]
            },
            "class_type": "CogVideoDecode"
        },
        "62": {
            "inputs": {
                "enable_tiling": False,
                "noise_aug_strength": 0,
                "strength": 1,
                "start_percent": 0,
                "end_percent": 1,
                "vae": ["59", 1],
                "start_image": ["37", 0]
            },
            "class_type": "CogVideoImageEncode"
        },
        "63": {
            "inputs": {
                "num_frames": num_frames,  # Use exact calculated frames
                "steps": 25,
                "cfg": 7.5,
                "seed": 1,
                "scheduler": "CogVideoXDDIM",
                "denoise_strength": 1.0,
                "model": ["59", 0],
                "positive": ["30", 0],
                "negative": ["31", 0],
                "image_cond_latents": ["62", 0]
            },
            "class_type": "CogVideoSampler"
        },
        "65": {                        # Added RIFE VFI node for frame interpolation
            "inputs": {
                "ckpt_name": "rife47.pth",
                "clear_cache_after_n_frames": 20,
                "multiplier": 2.0,      # Changed from 2.0 to 1.0 for more natural motion
                "fast_mode": False,
                "ensemble": True,
                "scale_factor": 1,
                "frames": ["60", 0]
            },
            "class_type": "RIFE VFI"
        }
    }
    
    return workflow

def merge_video_audio(video_path, audio_path, output_path):
    """Merge video and audio files."""
    try:
        # First get video duration
        video_duration_cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 {video_path}"
        video_duration = float(os.popen(video_duration_cmd).read().strip())
        
        # Get audio duration
        audio_duration_cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 {audio_path}"
        audio_duration = float(os.popen(audio_duration_cmd).read().strip())
        
        print(f"\nüé¨ Merging video and audio:")
        print(f"Video duration: {video_duration:.2f}s")
        print(f"Audio duration: {audio_duration:.2f}s")
        
        if video_duration > audio_duration:
            # If video is longer, use apad filter to add silence
            print("Video is longer than audio, adding silence padding...")
            merge_cmd = f"ffmpeg -i {video_path} -i {audio_path} -filter_complex '[1:a]apad[a1]' -map 0:v -map '[a1]' -c:v copy -c:a aac -shortest {output_path}"
        else:
            # If audio is longer or equal, just merge normally
            merge_cmd = f"ffmpeg -i {video_path} -i {audio_path} -c:v copy -c:a aac -shortest {output_path}"
        
        print(f"Executing merge command: {merge_cmd}")
        result = os.system(merge_cmd)
        time.sleep(2)  # Add delay after merge operation
        
        if result == 0:
            print(f"‚úÖ Successfully merged video and audio: {os.path.basename(output_path)}")
            time.sleep(1)  # Add delay after successful merge
            return True
        else:
            print("‚ùå Failed to merge video and audio")
            return False
            
    except Exception as e:
        print(f"‚ùå Error merging video and audio: {str(e)}")
        return False

def add_background_music(output_folder):
    """Add background music to the final movie with smooth transitions."""
    try:
        print("\n=== Adding Background Music ===")
        
        # Get video duration
        input_video = os.path.join(output_folder, "final_movie.mp4")
        cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 {input_video}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error getting video duration: {result.stderr}")
            return False
            
        video_duration = float(result.stdout.strip())
        print(f"Video duration: {video_duration:.2f} seconds")
        
        # Step 1: Process music file (convert to mono, match sample rate)
        print("Processing music file...")
        music_file = os.path.join(output_folder, "output.wav")
        processed_music = os.path.join(output_folder, "processed_music.wav")
        cmd = f"ffmpeg -i {music_file} -ac 1 -ar 22050 -acodec pcm_s16le {processed_music}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error processing music file: {result.stderr}")
            return False
            
        # Get music duration
        cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 {processed_music}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error getting music duration: {result.stderr}")
            return False
            
        music_duration = float(result.stdout.strip())
        print(f"Music duration: {music_duration:.2f} seconds")
        
        # Calculate number of loops needed
        num_loops = int(video_duration / music_duration) + 2  # Add 2 extra loops for safety
        print(f"Number of loops needed: {num_loops}")
        
        # Step 2: Add fades to music (2-second fade in/out)
        print("Adding fades to music...")
        faded_music = os.path.join(output_folder, "faded_music.wav")
        fade_duration = min(2.0, music_duration * 0.1)  # Use 10% of music duration or 2s, whichever is smaller
        fade_out_start = music_duration - fade_duration
        cmd = f"ffmpeg -i {processed_music} -af \"afade=t=in:st=0:d={fade_duration},afade=t=out:st={fade_out_start}:d={fade_duration}\" {faded_music}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error adding fades: {result.stderr}")
            return False
            
        # Step 3: Create loop list
        print("Creating music loop...")
        music_list = os.path.join(output_folder, "music_list.txt")
        with open(music_list, 'w') as f:
            for _ in range(num_loops):
                f.write("file 'faded_music.wav'\n")
            
        # Step 4: Concatenate faded music
        looped_music = os.path.join(output_folder, "looped_music_faded.wav")
        cmd = f"ffmpeg -f concat -safe 0 -i {music_list} -c copy {looped_music}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error concatenating music: {result.stderr}")
            return False
            
        # Step 5: Final mix (combine video with music at 15% volume)
        print("Mixing music with video...")
        output_video = os.path.join(output_folder, "final_movie_with_music_smooth.mp4")
        cmd = f"ffmpeg -i {input_video} -i {looped_music} -filter_complex \"[0:a][1:a]amerge=inputs=2,pan=stereo|c0=c0+0.15*c1|c1=c0+0.15*c1[a]\" -map 0:v -map \"[a]\" -c:v copy -c:a aac -b:a 192k {output_video}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"‚ùå Error mixing audio: {result.stderr}")
            return False
            
        # Clean up temporary files
        for temp_file in [processed_music, faded_music, music_list, looped_music]:
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
        print(f"‚úÖ Successfully added background music: {output_video}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error adding background music: {str(e)}")
        return False

def concatenate_videos(output_folder):
    """Concatenate all video files in the output folder"""
    try:
        # Get all final video files
        video_files = sorted(glob.glob(os.path.join(output_folder, "*_final.mp4")))
        if not video_files:
            print("‚ùå No video files found to concatenate")
            return False
            
        print(f"\nFound {len(video_files)} videos to concatenate")
        for video in video_files:
            print(f"  - {os.path.basename(video)}")
            
        # Create concat list file
        concat_list = os.path.join(output_folder, "concat_list.txt")
        with open(concat_list, 'w') as f:
            for video in video_files:
                f.write(f"file '{video}'\n")
                
        # Concatenate videos with proper audio handling
        output_file = os.path.join(output_folder, "final_movie.mp4")
        cmd = f"ffmpeg -f concat -safe 0 -i {concat_list} -c copy {output_file}"
        print(f"\nConcatenating videos with command: {cmd}")
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        time.sleep(5)  # Add significant delay after concatenation
        
        if result.returncode != 0:
            print(f"‚ùå Error concatenating videos: {result.stderr}")
            return False
            
        print(f"\n‚úÖ Successfully created final movie: {output_file}")
        return True
            
    except Exception as e:
        print(f"‚ùå Error in concatenate_videos: {str(e)}")
        return False

def estimate_text_duration(text):
    """Estimate the duration of text based on average speaking rate."""
    # Average speaking rate is about 150 words per minute
    # This means each word takes about 0.4 seconds
    words = len(text.split())
    estimated_duration = words * 0.4
    return estimated_duration

def adjust_text_for_duration(text, target_duration):
    """Adjust text to fit within target duration."""
    current_duration = estimate_text_duration(text)
    if current_duration <= target_duration:
        return text
        
    # If text is too long, try to make it more concise
    words = text.split()
    target_words = int(target_duration / 0.4)  # 0.4 seconds per word
    if target_words < 3:  # Minimum 3 words for coherence
        target_words = 3
        
    # Keep the most important words (start and end)
    if len(words) > target_words:
        kept_words = words[:target_words-2] + ["..."] + words[-2:]
        return " ".join(kept_words)
    return text

def generate_narration(text, image_path, output_folder, logger, character_data=None):
    """Generate narration audio using TTS API with intelligent voice selection"""
    try:
        base_filename = os.path.splitext(os.path.basename(image_path))[0]
        
        # Check if text is empty or just "..."
        if not text or text.strip() == "...":
            logger.info("Creating silent audio file for empty narration")
            # Get video duration to match silent audio length
            video_file = os.path.join(output_folder, f"{base_filename}_00001.mp4")
            audio_file = os.path.join(output_folder, f"{base_filename}_00001.wav")
            
            if os.path.exists(video_file):
                # Get video duration
                cmd = [
                    "ffprobe", "-v", "error", "-show_entries", "format=duration",
                    "-of", "default=noprint_wrappers=1:nokey=1", video_file
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                duration = float(result.stdout.strip())
                
                # Create silent audio file with same properties as TTS audio
                cmd = [
                    "ffmpeg", "-f", "lavfi", "-i", f"anullsrc=r=22050:cl=mono",
                    "-t", str(duration), "-acodec", "pcm_s16le", "-ar", "22050", "-b:a", "352800", audio_file
                ]
                subprocess.run(cmd, check=True)
                logger.info(f"Created silent audio file: {audio_file}")
                time.sleep(1)  # Add delay after file creation
                return True, "audio_generated"
            else:
                logger.warning(f"Video file not found: {video_file}")
                return False, "video_not_found"
        
        # Determine voice based on character gender
        voice = "me"  # Default voice
        if character_data and "base_traits" in character_data:
            base_traits = character_data["base_traits"].lower()
            print(f"\nüîç Analyzing character traits: {base_traits}")
            
            # Define gender indicators
            female_indicators = ["woman", "female", "girl", "lady", "she", "her"]
            male_indicators = ["man", "male", "boy", "gentleman", "he", "him"]
            
            # Check for gender indicators
            is_female = any(indicator in base_traits for indicator in female_indicators)
            is_male = any(indicator in base_traits for indicator in male_indicators)
            
            if is_female:
                # Randomly select from female voices
                female_voices = ["female1", "female2", "female3"]
                voice = random.choice(female_voices)
                print(f"üéôÔ∏è Selected female voice: {voice}")
            elif is_male:
                # Randomly select from male voices
                male_voices = ["male1", "male2", "male3"]
                voice = random.choice(male_voices)
                print(f"üéôÔ∏è Selected male voice: {voice}")
            else:
                print("‚ö†Ô∏è Could not determine gender from base_traits, using default voice")
        
        # For TTS API, we need 2 underscores total (base has 1, we add 1)
        filename = f"{base_filename}_00001_"  # Using 1 underscore since base has one
        
        # For actual TTS generation, use 2 underscores to match video pattern
        data = {
            "text": text,
            "voice": voice,
            "filename": filename,
            "filepath": output_folder
        }
        
        audio_file = os.path.join(output_folder, data['filename'] + '.wav')
        
        # Rest of the existing TTS API code...
        print(f"\nüéôÔ∏è Generating narration:")
        print(f"Text: {text}")
        print(f"Voice: {voice}")
        print(f"Output: {os.path.join(output_folder, data['filename'] + '.wav')}")
        
        # First, check if the file already exists
        if os.path.exists(audio_file) and os.path.getsize(audio_file) > 0:
            print(f"‚úÖ Audio file already exists: {audio_file}")
            file_size = os.path.getsize(audio_file)
            print(f"File size: {file_size/1024:.2f} KB")
            time.sleep(1)  # Add delay after file check
            return True, "audio_generated"
        
        # Send request to TTS API info endpoint first
        print("\nChecking TTS API info...")
        info_response = requests.post(TTS_API_URL, json=data)
        time.sleep(2)  # Add delay after API call
        
        if info_response.status_code != 200:
            print(f"‚ùå TTS API info check failed: {info_response.text}")
            return False, "tts_info_check_failed"
            
        # Now send request to TTS API
        print("\nSending request to TTS API...")
        response = requests.post(TTS_API_URL, json=data)
        time.sleep(2)  # Add delay after API call
        
        if response.status_code == 200:
            print(f"\n‚úÖ Generated narration: {data['filename']}")
            # Verify the audio file was created
            audio_file = os.path.join(output_folder, data['filename'] + '.wav')
            
            # Wait for the file to be created (with timeout)
            max_wait_time = 30  # seconds
            wait_interval = 1  # seconds
            waited = 0
            
            while not os.path.exists(audio_file) and waited < max_wait_time:
                print(f"Waiting for audio file to be created... ({waited}/{max_wait_time}s)")
                time.sleep(wait_interval)
                waited += wait_interval
            
            if os.path.exists(audio_file):
                file_size = os.path.getsize(audio_file)
                print(f"Audio file created: {audio_file}")
                print(f"File size: {file_size/1024:.2f} KB")
                
                # Add a small delay after successful generation to prevent overwhelming the API
                time.sleep(3)  # Increased delay after successful generation
                
                return True, "audio_generated"
            else:
                print(f"‚ùå Audio file not found at: {audio_file} after waiting {max_wait_time} seconds")
                return False, "audio_file_not_found"
        else:
            print(f"\n‚ùå Failed to generate narration:")
            print(f"Error: {response.text}")
            return False, "tts_generation_failed"
            
    except Exception as e:
        print(f"‚ùå Error generating narration: {str(e)}")
        return False, str(e)

def setup_detailed_logging(folder_id):
    """Set up detailed logging for the video generation process"""
    # Create logs directory if it doesn't exist
    logs_dir = os.path.join(COMFYUI_OUTPUT_DIR, folder_id, "logs")
    os.makedirs(logs_dir, exist_ok=True)
    
    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(logs_dir, "video_generation.log")),
            logging.StreamHandler()
        ]
    )
    
    # Disable debug logging for requests and urllib3
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    # Comment out the following line to disable ComfyUI API polling debug logs
    # logging.getLogger("http.client").setLevel(logging.DEBUG)
    
    return logging.getLogger(__name__)

def log_processing_stats(logger, sequence_data, processed_scenes):
    """Log detailed processing statistics"""
    logger.info("\n=== Processing Statistics ===")
    logger.info(f"Total scenes in sequence: {len(sequence_data)}")
    logger.info(f"Successfully processed scenes: {len(processed_scenes)}")
    
    # Log scene-by-scene breakdown
    for item in sequence_data:
        scene_key = f"{item['sequence_number']}_{item['type']}"
        status = "Processed" if scene_key in processed_scenes else "Skipped"
        logger.info(f"Scene {item['sequence_number']} ({item['type']}): {status}")
    
    logger.info("=== End Statistics ===\n")

def generate_music_score(output_folder, music_score):
    """Generate background music for the final movie."""
    try:
        print("\nüéµ Generating Music Score:")
        print(f"Style: {music_score.get('style', 'N/A')}")
        print(f"Type: {music_score.get('type', 'N/A')}")
        print(f"Instrumentation: {music_score.get('instrumentation', 'N/A')}")
        print(f"Tempo: {music_score.get('tempo', 'N/A')}")
        print(f"Duration: 95 seconds")
        
        # Format music score as a string
        music_prompt = (
            f"Style: {music_score.get('style', '')}, "
            f"Type: {music_score.get('type', '')}, "
            f"Instrumentation: {music_score.get('instrumentation', '')}, "
            f"Tempo: {music_score.get('tempo', '')}"
        )
        
        # Prepare the request payload
        payload = {
            "ref_prompt": music_prompt,
            "audio_length": 95,  # Hardcoded to 95 seconds
            "repo_id": "ASLP-lab/DiffRhythm-base",
            "output_dir": output_folder,
            "chunked": True
        }
        
        # Send request to music generation server
        print("\nSending request to music generation server...")
        response = requests.post(MUSIC_GEN_API_URL, json=payload)
        
        if response.status_code != 200:
            print(f"‚ùå Music generation failed with status code: {response.status_code}")
            print(f"Response: {response.text}")
            return False
            
        # Check for output file
        output_file = os.path.join(output_folder, "output.wav")
        max_wait_time = 300  # 5 minutes timeout
        wait_interval = 5  # Check every 5 seconds
        waited = 0
        
        print("\n‚è≥ Waiting for music generation to complete...")
        while not os.path.exists(output_file) and waited < max_wait_time:
            print(f"Waiting... ({waited}/{max_wait_time}s)")
            time.sleep(wait_interval)
            waited += wait_interval
        
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"\n‚úÖ Music generated successfully: {output_file}")
            print(f"File size: {file_size/1024/1024:.2f} MB")
            return True
        else:
            print(f"‚ùå Music file not found after {max_wait_time} seconds")
            return False
            
    except Exception as e:
        print(f"‚ùå Error generating music: {str(e)}")
        return False

def process_video_generation(folder_id, data):
    """Process video generation for a sequence of images in strict sequential order."""
    output_folder = os.path.join(COMFYUI_OUTPUT_DIR, folder_id)
    
    if not data or "sequence" not in data:
        print("‚ùå No sequence data provided")
        return {"status": "error", "message": "No sequence data provided"}
    
    sequence_data = data["sequence"]
    print(f"\nStarting video generation for {len(sequence_data)} scenes...")
    print(f"Output folder: {output_folder}")
    
    # Debug sequence data
    print("\nSequence Data Structure:")
    print(json.dumps(sequence_data, indent=2))
    
    # Phase 1: Generate background music
    print("\n=== Phase 1: Generating Background Music ===")
    music_score = data.get("music_score")
    
    if music_score:
        print("Found music score in data")
        success = generate_music_score(output_folder, music_score)
        if not success:
            return {"status": "error", "message": "Failed to generate background music"}
    else:
        print("No music score found in data")
    
    # Phase 2: Generate all videos
    print("\n=== Phase 2: Generating Videos ===")
    for item in sequence_data:
        scene_number = item.get("sequence_number")
        if not scene_number:
            continue
            
        print(f"\nProcessing scene {scene_number}...")
        base_name = f"scene_{format_sequence_number(scene_number)}_{item.get('type', 'character')}_00001_"
        video_file = os.path.join(output_folder, f"{base_name}__00001.mp4")
        
        success = generate_video(
            os.path.join(output_folder, f"{base_name}.png"),
            video_file,
            item.get("clip_duration", 3.0625),
            item.get("clip_action")
        )
        
        if not success:
            return {"status": "error", "message": f"Failed to generate video for scene {scene_number}"}
    
    # Phase 3: Generate all audio
    print("\n=== Phase 3: Generating Audio ===")
    for item in sequence_data:
        if "voice_narration" not in item:
            continue
            
        scene_number = item.get("sequence_number")
        base_name = f"scene_{format_sequence_number(scene_number)}_{item.get('type', 'character')}_00001_"
        
        print(f"\nGenerating audio for scene {scene_number}...")
        success = generate_narration(
            item["voice_narration"],
            os.path.join(output_folder, f"{base_name}.png"),
            output_folder,
            setup_detailed_logging(folder_id),
            data.get("character")  # Pass the character data
        )
        
        if not success:
            return {"status": "error", "message": f"Failed to generate audio for scene {scene_number}"}
    
    # Phase 4: Merge videos with audio
    print("\n=== Phase 4: Merging Videos with Audio ===")
    video_files = sorted(glob.glob(os.path.join(output_folder, "scene_*_*_00001__00001.mp4")))
    merged_videos = []
    
    for video_file in video_files:
        base_name = os.path.splitext(os.path.basename(video_file))[0].replace("__00001", "")
        merged_output = os.path.join(output_folder, f"{base_name}_final.mp4")
        
        print(f"\nProcessing video: {base_name}")
        
        audio_patterns = [
            os.path.join(output_folder, f"{base_name}__00001.wav"),
            os.path.join(output_folder, f"{base_name}__00001_.wav"),
            os.path.join(output_folder, f"{base_name}___00001_.wav")
        ]
        
        audio_file = None
        for pattern in audio_patterns:
            if os.path.exists(pattern):
                audio_file = pattern
                print(f"Found audio file: {pattern}")
                break
        
        if audio_file:
            print(f"Merging with audio file: {audio_file}")
            success = merge_video_audio(video_file, audio_file, merged_output)
            if success:
                merged_videos.append(merged_output)
        else:
            print(f"No audio file found for: {base_name} - skipping")
            continue
    
    # Phase 5: Concatenate all scenes
    print("\n=== Phase 5: Concatenating All Scenes ===")
    success = concatenate_videos(output_folder)
    
    if not success:
        return {"status": "error", "message": "Failed to concatenate videos"}
    
    # Phase 6: Add background music
    print("\n=== Phase 6: Adding Background Music ===")
    if music_score:
        success = add_background_music(output_folder)
        if not success:
            return {"status": "error", "message": "Failed to add background music"}
        print("‚úÖ Background music added successfully")
    
    print("\n‚úÖ All video generation phases completed successfully")
    return {"status": "success", "message": "Video generation completed"}

def generate_video(image_path, output_path, clip_duration=5, clip_action=None):
    """Generate a video from an image using ComfyUI."""
    try:
        # Calculate frames based on duration + 1 second buffer
        FPS = 24
        buffered_duration = clip_duration + 0.5  # Add 1 second buffer
        num_frames = int(buffered_duration * FPS)
        
        # Get the base filename without extension
        base_filename = os.path.splitext(os.path.basename(image_path))[0]
        
        # Use the provided clip_action for the positive prompt
        positive_prompt = clip_action if clip_action else "cinematic shot, high quality, detailed, sharp focus"
        
        # Log the prompts and timing
        print("\nüé¨ Video Generation Settings:")
        print(f"Input image: {image_path}")
        print(f"Output path: {output_path}")
        print(f"Positive Prompt: {positive_prompt}")
        print(f"Original Duration: {clip_duration}s")
        print(f"Buffered Duration: {buffered_duration}s")
        print(f"Total Frames: {num_frames}")
        print(f"Timeout: {VIDEO_GENERATION_TIMEOUT} seconds")
        
        # Build the workflow
        workflow = build_video_workflow(
            image_path,
            positive_prompt,
            os.path.dirname(output_path),
            clip_duration=clip_duration
        )
        
        # Check if video already exists and is valid
        if check_video(os.path.dirname(output_path), base_filename):
            print(f"‚úÖ Video already exists and is valid: {base_filename}")
            return True
            
        # Send request to ComfyUI API
        print("\nSending request to ComfyUI API...")
        response = requests.post(COMFYUI_API_URL, json={"prompt": workflow})
        
        if response.status_code != 200:
            print(f"‚ùå API request failed with status code: {response.status_code}")
            print(f"Response: {response.text}")
            return False
            
        try:
            response_data = response.json()
            prompt_id = response_data.get("prompt_id")
            if not prompt_id:
                print("‚ùå No prompt_id in response")
                return False
                
            # Wait for the video generation to complete
            print("\n‚è≥ Waiting for video generation to complete...")
            start_time = time.time()
            last_progress = 0
            last_status = None
            completion_detected = False
            
            while time.time() - start_time < VIDEO_GENERATION_TIMEOUT:
                # Check queue status
                queue_response = requests.get("http://127.0.0.1:8188/queue")
                if queue_response.status_code == 200:
                    queue_data = queue_response.json()
                    if "queue_running" in queue_data and prompt_id in queue_data["queue_running"]:
                        elapsed_time = int(time.time() - start_time)
                        current_status = f"üé¨ Video generation in progress... ({elapsed_time}/{VIDEO_GENERATION_TIMEOUT}s)"
                        if current_status != last_status:
                            print(current_status)
                            last_status = current_status
                        time.sleep(2)
                        continue
                
                # Check history status
                history_response = requests.get("http://127.0.0.1:8188/history")
                if history_response.status_code == 200:
                    history_data = history_response.json()
                    if prompt_id in history_data:
                        print("‚úÖ Video generation completed!")
                        time.sleep(2)
                        if check_video(os.path.dirname(output_path), base_filename):
                            completion_detected = True
                            break
                
                # Check file growth
                video_patterns = [
                    os.path.join(os.path.dirname(output_path), f"{base_filename}.mp4"),
                    os.path.join(os.path.dirname(output_path), f"{base_filename}_00001.mp4"),
                    os.path.join(os.path.dirname(output_path), f"{base_filename}__00001.mp4")
                ]
                
                for video_path in video_patterns:
                    if os.path.exists(video_path):
                        current_size = os.path.getsize(video_path)
                        if current_size > last_progress:
                            elapsed_time = int(time.time() - start_time)
                            print(f"üìä Video file growing: {current_size/1024:.1f} KB ({elapsed_time}/{VIDEO_GENERATION_TIMEOUT}s)")
                            last_progress = current_size
                            last_status = f"üìä Video file growing: {current_size/1024:.1f} KB"
                
                time.sleep(2)
            
            if not completion_detected:
                print(f"‚ùå Video generation timed out after {VIDEO_GENERATION_TIMEOUT} seconds")
                return False
                
            # Add a small delay after completion to ensure file system sync
            time.sleep(2)
            return True
                
        except json.JSONDecodeError:
            print("‚ùå Failed to parse API response as JSON")
            print(f"Raw response: {response.text[:500]}...")
            return False

    except Exception as e:
        print(f"‚ùå Error generating video: {str(e)}")
        return False

def upload_video_to_firebase(video_path, movie_id):
    """Upload video to Firebase Storage and return the public URL."""
    try:
        # Extract filename from path
        filename = os.path.basename(video_path)
        
        # Create storage path: movies/{movie_id}/videos/{filename}
        storage_path = f"movies/{movie_id}/videos/{filename}"
        
        # Upload the video
        print(f"\nüì§ Uploading video to Firebase Storage: {storage_path}")
        storage.child(storage_path).put(video_path)
        
        # Get the public URL
        video_url = storage.child(storage_path).get_url(None)
        print(f"‚úÖ Video uploaded successfully: {video_url}")
        
        return video_url
    except Exception as e:
        print(f"‚ùå Error uploading video to Firebase: {str(e)}")
        raise

def update_firestore_with_video_url(movie_id, video_url):
    """Update Firestore document with the video URL."""
    try:
        # Get the movie document reference
        movie_ref = db.collection('movies').document(movie_id)
        
        # Update the document with the video URL
        print(f"\nüìù Updating Firestore with video URL for movie: {movie_id}")
        movie_ref.update({
            'final_video': video_url,
            'updated_at': firestore.SERVER_TIMESTAMP
        })
        
        print("‚úÖ Firestore updated successfully")
    except Exception as e:
        print(f"‚ùå Error updating Firestore: {str(e)}")
        raise

@app.route("/generateVideos/<folder_id>", methods=["POST"])
def generate_videos(folder_id):
    """API endpoint to generate images for a sequence of shots."""
    try:
        data = request.get_json()
        if not data or "sequence" not in data:
            return jsonify({"status": "error", "message": "No sequence data provided"}), 400
            
        # Set up detailed logging
        logger = setup_detailed_logging(folder_id)
        logger.info(f"Starting video generation for folder: {folder_id}")
        
        # Process videos with detailed logging, passing the entire data object
        result = process_video_generation(folder_id, data)
        
        # Log final statistics
        logger.info("Video generation process completed")
        logger.info(f"Final result: {result}")
        
        # Get the final video path from the output folder
        output_folder = os.path.join(COMFYUI_OUTPUT_DIR, folder_id)
        final_video_path = os.path.join(output_folder, "final_movie_with_music_smooth.mp4")
        
        if not os.path.exists(final_video_path):
            return jsonify({"error": "Final video not found"}), 500

        # Upload video to Firebase using folder_id as movie_id
        try:
            video_url = upload_video_to_firebase(final_video_path, folder_id)
            update_firestore_with_video_url(folder_id, video_url)
        except Exception as e:
            logger.error(f"Error uploading to Firebase: {str(e)}")
            return jsonify({"error": "Failed to upload video to Firebase"}), 500

        return jsonify({
            "message": "Video generated and uploaded successfully",
            "video_url": video_url
        }), 200

    except Exception as e:
        logger.error(f"Error in video generation: {str(e)}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001, debug=True) 